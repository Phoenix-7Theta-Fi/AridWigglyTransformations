{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzKLOmgmmxNo4S7UbF5I0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phoenix-7Theta-Fi/AridWigglyTransformations/blob/main/3rd_October.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "By_dWE0HSeDP"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install -q langchain-google-genai langchain-qdrant qdrant-client\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Set the Google API key\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCAkNL5AENDygdCFgx3E2ASH8n3tVeORHY\"\n",
        "\n",
        "# Initialize Google Generative AI (Gemini 1.5 Pro) for both Triage and Diagnostic agents\n",
        "triage_agent = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "diagnostic_agent = GoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "WU627AaKTXik"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports for Qdrant and Google embeddings\n",
        "from qdrant_client import QdrantClient\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "# Set the Google API key and Qdrant API key directly from the provided credentials\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCAkNL5AENDygdCFgx3E2ASH8n3tVeORHY\"  # Provided Google API key\n",
        "\n",
        "QDRANT_API_KEY = \"Zuve0HmtPgefnl5zHWf6GSmh1IM1hasHqdeqIG_0ujUxjXq0G1Jq3Q\"\n",
        "QDRANT_URL = \"https://229b929d-06ae-4ee8-83f0-5d7bee918989.europe-west3-0.gcp.cloud.qdrant.io\"  # Provided Qdrant URL\n",
        "\n",
        "# Initialize Qdrant client using the cloud URL and API key\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "\n",
        "# Initialize Google Generative AI Embeddings model for vectorization\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# Use the existing \"ayurveda_collection\" in Qdrant\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"ayurveda_collection\",  # Provided collection name\n",
        "    embedding=embeddings,  # Embedding model for semantic search\n",
        ")\n"
      ],
      "metadata": {
        "id": "4jtE5hpSUuJe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize conversation buffer for managing consultation context\n",
        "conversation_memory = ConversationBufferMemory()\n"
      ],
      "metadata": {
        "id": "qfocpCZmVh_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize the memory buffer for managing the conversation context\n",
        "conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Define the system message template for the Triage Agent\n",
        "triage_system_prompt = \"\"\"\n",
        "You are an Ayurvedic health assistant conducting an interview to gather a comprehensive health history.\n",
        "Ask questions to assess the user's health concerns, symptoms, lifestyle, and other relevant Ayurvedic health factors.\n",
        "Formulate the next question based on the user's previous answers, and avoid repetition.\n",
        "\n",
        "You can ask about:\n",
        "- The reason for visiting\n",
        "- Current symptoms\n",
        "- Lifestyle habits (diet, sleep, stress levels)\n",
        "- Ayurvedic Dosha imbalances (if relevant)\n",
        "- Medical history (Ayurvedic or general)\n",
        "\"\"\"\n",
        "\n",
        "# Define the conversation prompt\n",
        "triage_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", triage_system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Use LLMChain to handle interaction between the user and the agent\n",
        "triage_agent_chain = LLMChain(\n",
        "    llm=triage_agent,  # Your initialized GoogleGenerativeAI (Gemini) model\n",
        "    prompt=triage_prompt,\n",
        "    memory=conversation_memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Function to conduct the interview and generate the report\n",
        "def conduct_interview_and_generate_report(triage_agent_chain, vector_store, max_questions=15):\n",
        "    print(\"Welcome to the Ayurvedic Health Assessment. Let's start with a few questions.\")\n",
        "\n",
        "    for i in range(max_questions):\n",
        "        # Ask the user the next question generated by the AI\n",
        "        user_input = input(f\"User [{i+1}]: \")  # Simulate user input, in a real app this would be dynamic user input\n",
        "        response = triage_agent_chain.run(input=user_input)  # Ensure input structure is correct\n",
        "\n",
        "        # Optionally, retrieve relevant Ayurvedic knowledge using Qdrant\n",
        "        retrieved_info = vector_store.similarity_search_with_score(user_input)\n",
        "\n",
        "        # Print the agent's response (the next question or follow-up)\n",
        "        print(f\"Agent [{i+1}]: {response}\")\n",
        "\n",
        "        # Optionally, show the relevant Ayurvedic knowledge for user info\n",
        "        if retrieved_info:\n",
        "            print(f\"Relevant Ayurvedic Insight: {retrieved_info[0][0].page_content}\")\n",
        "\n",
        "        # The agent checks if it's ready to generate the report\n",
        "        if \"Is there anything else you'd like to add?\" in response or \"Do you feel like we've covered everything?\" in response:\n",
        "            user_confirmation = input(\"User [confirmation]: \").lower()\n",
        "\n",
        "            if user_confirmation in [\"yes\", \"y\", \"that's everything\", \"i'm done\"]:\n",
        "                print(\"\\nGenerating report based on the gathered information...\\n\")\n",
        "\n",
        "                # Generate and display the report\n",
        "                report = generate_report(triage_agent_chain.memory)\n",
        "                print(report)\n",
        "                return report  # Return report for use in Agent 2\n",
        "\n",
        "            # If the user says there's more to add, the loop continues for additional questions.\n",
        "            print(\"Okay, let's continue with the assessment.\")\n",
        "\n",
        "    # If the max_questions is reached, the agent concludes the interview.\n",
        "    print(\"\\nMax number of questions reached. Generating report...\\n\")\n",
        "    report = generate_report(triage_agent_chain.memory)\n",
        "    print(report)\n",
        "    return report\n",
        "\n",
        "# Helper function to generate the report based on the conversation history\n",
        "def generate_report(memory):\n",
        "    # Retrieve the chat history (memory of the conversation)\n",
        "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
        "\n",
        "    # Generate a simple report summary from the chat history\n",
        "    report = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in chat_history])\n",
        "\n",
        "    return f\"\\nHere is a summary of your health assessment:\\n\\n{report}\"\n",
        "\n",
        "# Run the full workflow: conduct interview and generate report\n",
        "report = conduct_interview_and_generate_report(triage_agent_chain, vector_store)\n",
        "\n",
        "# Placeholder function to send the report to Agent 2\n",
        "def send_to_agent_2(report, diagnostic_agent):\n",
        "    print(\"\\nSending report to Diagnostics and Treatment Planning Agent...\\n\")\n",
        "\n",
        "    # Use the diagnostic agent to analyze the report and provide a diagnosis\n",
        "    diagnosis_input = {\"input\": report}\n",
        "    diagnosis = diagnostic_agent.invoke(diagnosis_input)\n",
        "\n",
        "    # Display the diagnosis and treatment recommendations\n",
        "    print(\"Diagnosis and Treatment Plan:\")\n",
        "    print(diagnosis)\n",
        "\n",
        "# Once the report is generated, you can send it to Agent 2 for diagnosis and treatment planning\n",
        "send_to_agent_2(report, diagnostic_agent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "0wtV3cFmP5r4",
        "outputId": "dcf9e372-20e3-4f9e-dd6e-9089e18eeedc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Ayurvedic Health Assessment. Let's start with a few questions.\n",
            "User [1]: head ache and nausea\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: \n",
            "You are an Ayurvedic health assistant conducting an interview to gather a comprehensive health history.\n",
            "Ask questions to assess the user's health concerns, symptoms, lifestyle, and other relevant Ayurvedic health factors.\n",
            "Formulate the next question based on the user's previous answers, and avoid repetition.\n",
            "\n",
            "You can ask about:\n",
            "- The reason for visiting\n",
            "- Current symptoms\n",
            "- Lifestyle habits (diet, sleep, stress levels)\n",
            "- Ayurvedic Dosha imbalances (if relevant)\n",
            "- Medical history (Ayurvedic or general)\n",
            "\n",
            "Human: head ache and nausea\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent [1]: I understand you're experiencing a headache and nausea.  Could you tell me a bit more about these symptoms? For instance, where is the headache located, and what does the nausea feel like? \n",
            "\n",
            "Relevant Ayurvedic Insight: aching head pain; a feeling of hot coals on the scalp;burning vapor from nostrils (diminishes at night or when\n",
            "applying cold packs to the head).\n",
            "Kapha: Headaches; a sticky mucus-coated palate and\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c00c0742ceaf>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Run the full workflow: conduct interview and generate report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconduct_interview_and_generate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriage_agent_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Placeholder function to send the report to Agent 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c00c0742ceaf>\u001b[0m in \u001b[0;36mconduct_interview_and_generate_report\u001b[0;34m(triage_agent_chain, vector_store, max_questions)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Ask the user the next question generated by the AI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"User [{i+1}]: \"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Simulate user input, in a real app this would be dynamic user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriage_agent_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure input structure is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}